# LOADING THE DATA TO GOOGLE CLOUD

# create a bucket in Google Storage, install Google Cloud SDK
import os
from google.cloud import storage
# Create a service account in Gogle Cloud
# Grant Storage Object Creator role
# Create a key and download it onto your local disc
# Path to the key
key = "C:\\Users\\Acer\\Desktop\\programming\\onwelo\\onwelo-401512-a3f4c363853d.json"
# Authenticate with Google Cloud Platfor
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = key
client = storage.Client()

# Define a function uploading the files form local disc

from google.cloud import storage
def upload_to_gcs(bucket_name, source_file_path, destination_blob_name, key):
    # Initialize the Google Cloud Storage client with the credentials
    storage_client = storage.Client.from_service_account_json(key)

    # Get the target bucket
    bucket = storage_client.bucket(bucket_name)

    # Upload the file to the bucket
    blob = bucket.blob(destination_blob_name)
    blob.upload_from_filename(source_file_path)

    print(f"File {source_file_path} uploaded to gs://{bucket_name}/{destination_blob_name}")

# Download data from github and save as .json

import pandas as pd
names = ['customers', 'employees', 'orders', 'productcategories', 'products', 'productsubcategories', 'vendorproduct', 'vendors']
bucket_name = 'case_study_data1'
key = "C:\\Users\\Acer\\Desktop\\programming\\onwelo\\onwelo-401512-a3f4c363853d.json"

for name in names:
    url = 'https://raw.githubusercontent.com/sfrechette/adventureworks-neo4j/master/data/' + name + '.csv'
    df = pd.read_csv(url, encoding='latin-1')
    df.to_json(name)
    source_file_path = 'C:\\Users\\Acer\\Desktop\\programming\\onwelo\\venv\\Lib\\' + name

    # Use the upload_to_gcs function to upload the data
    upload_to_gcs(bucket_name, source_file_path, name, key)

#LOADING THE DATA TO BUG QUERY

# Grant BigQuery Job Use role to earlier created service account and create a key

import os
key2 = "C:\\Users\\Acer\\Desktop\\programming\\onwelo\\onwelo-401512-a3f4c363853d.json"
#authenticate with Google Cloud Platform
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = key2

#Create a BigQuery database
from google.cloud import bigquery

# Construct a BigQuery client object
client = bigquery.Client()

# Set dataset_id to the ID of the dataset to create
dataset_id = "{}.case_study".format(client.project)

# Construct a full Dataset object to send to the API
dataset = bigquery.Dataset(dataset_id)

# Specify the geographic location where the dataset should reside (deafult option is US)
dataset.location = "US"

# Send the dataset to the API for creation, with an explicit timeout
# Raises google.api_core.exceptions.Conflict if the Dataset alread exists within the project
dataset = client.create_dataset(dataset, timeout=30)

# Make an API request.]
print("Created dataset {}.{}".format(client.project, dataset.dataset_id))

# Generate a json schema from .json files and save them as schema + name

# Add a function uploading the data to BigQuery

def storage_to_data(key, project, dataset, table, schema, uri):
    # Construct a BigQuery client object
    client = bigquery.Client()

    # Set table_id to the ID of the table to create
    table_id = project + '.' + dataset + '.' + table
    job_config = bigquery.LoadJobConfig(
        schema=schema,
        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,
    )
    load_job = client.load_table_from_uri(
        uri,
        table_id,
        location="US",
        # Must match the destination dataset location
        job_config=job_config,
    )
    # Make an API request
    load_job.result()
    # Waits for the job to complete
    destination_table = client.get_table(table_id)
    print("Loaded {} rows.".format(destination_table.num_rows))

# Run a loop for each file

project = 'onwelo-401512'
dataset = 'case_study'
names = ['customers', 'employees', 'orders', 'productcategories', 'products', 'productsubcategories', 'vendorproduct', 'vendors']

for name in names:
    json_file_path = "C:\\Users\\Acer\\Desktop\\programming\\onwelo\\venv\\Lib\\schema" + name
    schema = generate_schema_from_file(json_file_path)
    uri = ('gs://case_study_data1 /" + name'
    # Upload the data to BigQuery database
    storage_to_data(key2, project, dataset, table, schema, uri)




